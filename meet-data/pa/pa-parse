#!/bin/bash

set -e

if [ $# -ne 1 ]; then
	echo " Usage: $0 http://url/to/results/page"
	exit 1
fi

SCRIPTDIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
REPOSCRIPTDIR="${SCRIPTDIR}/../../scripts"

# Download the website to original.html.
wget --output-document=original.html "$1"

# Save the website URL for future use, since it's pretty hard to tell
# what meets on the site are tracked and which aren't.
echo "$1" > URL

# Sometimes the documents are encoded in ISO-8859-1.
file original.html | grep ISO-8859 && iconv -f ISO-8859-1 -t UTF-8 original.html > original2.html
if [ -f original2.html ]; then
	mv original2.html original.html
fi

# Extract just the original table from original.html.
# Save it as original.xls so that libreoffice can convert to csv.
${SCRIPTDIR}/pa-extract-table original.html > original.xls

# Replace any commas in the xls file with periods.
# Commas are used as decimal points in the European style.
sed -i -e 's/,/\./g' original.xls

# Use LibreOffice to automatically convert the <table> to a csv file.
# This creates original.csv.
libreoffice --headless --convert-to csv original.xls

# Failed lifts are marked as "200x" or "202.5x". Replace with negatives.
sed -i -e 's/,\([0-9][0-9]*\)x/,-\1/g' original.csv
sed -i -e 's/,\([0-9][0-9]*\.[0-9][0-9]*\)x/,-\1/g' original.csv
sed -i -e 's/,\([0-9][0-9]*\)X/,-\1/g' original.csv
sed -i -e 's/,\([0-9][0-9]*\.[0-9][0-9]*\)X/,-\1/g' original.csv

# Some nonsense that gets added all the way on the right.
sed -i -e 's/,sort on/,/g' original.csv

# If CSV conversion completed successfully, remove the intermediary
# files early to benefit terminal autocompletion.
if [ -f original.csv ]; then
	rm original.html original.xls
fi

# Commands after this point were extracted into a separate file
# since they had to be re-run by hand in case of error.
${SCRIPTDIR}/pa-parse-post
