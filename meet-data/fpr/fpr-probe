#!/usr/bin/env python3
# vim: set ts=8 sts=4 et sw=4 tw=99:
#
# Probes for new meets from the FPR
#

from bs4 import BeautifulSoup
import os
import sys

try:
    import oplprobe
except ModuleNotFoundError:
    sys.path.append(os.path.join(os.path.dirname(os.path.dirname(
        os.path.dirname(os.path.realpath(__file__)))), "scripts"))
    import oplprobe


URLS = ['http://fpr-info.ru/protokol.htm',
        'http://fpr-info.ru/___protokoly/prot_2017/prot_2017.htm',
        'http://fpr-info.ru/___protokoly/prot_2016/prot_2016.htm',
        'http://fpr-info.ru/___protokoly/prot_2015/prot_2015.htm',
        'http://fpr-info.ru/___protokoly/prot_2014/prot_14.htm',
        'http://fpr-info.ru/___protokoly/prot_2013/prot_13.htm',
        'http://fpr-info.ru/___protokoly/prot_2012/prot_12.htm',
        'http://fpr-info.ru/___protokoly/prot_2011/prot_11.htm',
        'http://fpr-info.ru/___protokoly/prot_2010/prot_10.htm',
        'http://fpr-info.ru/___protokoly/prot_2009/prot_09.htm',
        'http://fpr-info.ru/___protokoly/prot_2008/prot_08.htm',
        'http://fpr-info.ru/___protokoly/prot_2007/prot_07.htm']
FEDDIR = os.path.dirname(os.path.realpath(__file__))


def colour(s):
    return "\033[1;33m" + s + "\033[0;m"


# FPR has malformed html on their results page, remove the extra tags
def fix_html(html):
    tag_dict = {}
    remove_tags = []

    ii = 0
    split_html = html.split('<')

    # Find unopened tags
    for tag_start in split_html:
        if tag_start != '':
            if tag_start[0] == '/':  # Close tag
                close_idx = tag_start.find(">")
                tag_type = tag_start[1:tag_start.find(">")]
                if tag_type not in tag_dict:
                    remove_tags.append(ii)
                elif tag_dict[tag_type] == 0:
                    remove_tags.append(ii)
                else:
                    tag_dict[tag_type] -= 1

            elif '/>' not in tag_start and tag_start[0] != 'p':  # Open tag
                close_idx = tag_start.find(">")
                tag_type = tag_start[0:tag_start.find(">")].split(" ")[0]
                if tag_type not in tag_dict:
                    tag_dict[tag_type] = 1
                else:
                    tag_dict[tag_type] += 1
        ii = ii + 1

    # Remove the unopened tags
    for idx in remove_tags:
        close_idx = split_html[idx].find(">")
        if len(split_html[idx]) > close_idx + 1:
            split_html[idx] = split_html[idx][close_idx + 1:]
        else:
            split_html[idx] = ''

    split_html = ["<" + line for line in split_html if line != '']
    return ''.join(split_html)


def getmeetlist(html, main_url):
    base_url = main_url.rsplit('/', 1)[0]

    html = fix_html(str(html, 'windows-1251'))

    soup = BeautifulSoup(html, 'html.parser')
    outfile = open('text.html', 'w')
    outfile.write(soup.prettify())
    outfile.close()
    divs = soup.find_all("table", {"width": "101%"})

    # Pre 2010 the tables are slightly different
    if divs == []:
        divs = soup.find_all("table", {"id": "table2"})  # 07
    if divs == []:
        divs = soup.find_all("table", {"id": ["table7", "table4"]})  # 08
    if divs == []:
        divs = soup.find_all("table", {"id": ["table11", "table12"]})  # 09

    urls = []
    for div in divs:
        for a in div.find_all('a'):
            url = a['href']

        # Non relative links are meet photos and international meets, mir
        # =Worlds,evr =Euros
            if not any(test_str in url for test_str in['http', 'mir', 'arnold', 'evr']):
                url = base_url + "/" + url
                url = url.replace('.com//', '/')
                if url not in urls:
                    urls.append(url)
    return urls


def main():

    meetlist = []
    for url in URLS:
        html = oplprobe.gethtml(url)
        meetlist = meetlist + getmeetlist(html, url)

    entered = oplprobe.getenteredurls(FEDDIR)
    unentered = oplprobe.getunenteredurls(meetlist, entered)

    oplprobe.print_meets(colour('[RPU]'), unentered)


if __name__ == '__main__':
    main()
