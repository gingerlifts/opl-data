# !/usr/bin/env python3
# vim: set ts=8 sts=4 et sw=4 tw=99:
#
# Import data from the AllPowerlifting website. This script should only be used
# for meets where AllPowerlifting is the primary source.

from bs4 import BeautifulSoup
import errno
from oplcsv import Csv
import os
import sys
import urllib.request
import re


def gethtml(url):
    with urllib.request.urlopen(url) as r:
        return r.read().decode('utf-8')


def error(msg):
    print(msg, file=sys.stderr)
    sys.exit(1)


# URL will be like "/?page=protokoll_vis&id=3508".
def getdirname(url):
    dirname = url.split('results/')[1][:-1]
    dirname = dirname.replace('/', '-')
    return dirname


def getmeetinfo(soup):
    month_dict = {'january': '01', 'febuary': '02', 'march': '03', 'april': '04',
                  'may': '05', 'june': '06', 'july': '07', 'august': '08',
                  'september': '09', 'october': '10', 'november': '11',
                  'december': '12'}

    csv = Csv()
    csv.fieldnames = ['Federation', 'Date', 'MeetCountry',
                      'MeetState', 'MeetTown', 'MeetName']

    # Get the info table.
    infotable = soup.find('table', {'class': 'stdtable'})
    if infotable == []:
        error("Couldn't find the info table.")

    # Get the fed.
    fedrow = infotable.find_all('tr')[0]
    fed = fedrow.find_all('td')[1].text
    fed = fed[fed.find("(")+1:fed.find(")")]

    # Get the location.
    venuerow = fedrow.find_next_sibling()
    venue = venuerow.find_all('td')[1].text
    if venue[-2:] == ', ':
        venue = venue[:-2]

    split = venue.split(', ')
    if len(split) > 1:
        country = split[0]
        town = split[-1]
    else:
        country = venue
        town = ''

    # Get the date.
    daterow = venuerow.find_next_sibling()
    longdate = daterow.find_all('td')[1].text
    [daymonth, year] = longdate.split(', ')

    month = daymonth.split(' ')[0]
    day = daymonth.split(' ')[1]

    for month_key in month_dict.keys():
        if month.lower() in month_key:
            month = month_dict[month_key]
            break

    date = year + '-' + month + '-' + day

    # Get the competition name.
    h1 = soup.find('h1')
    if len(h1) != 1:
        error("Couldn't find the competition name.")
    name = h1.text

    state = ''

    row = [fed, date, country, state, town, name]
    for i, r in enumerate(row):
        row[i] = r.replace(',', ' ').replace('  ', ' ').strip()
    csv.rows = [row]
    return csv


def getresults(soup, url):
    csv = Csv()

    subpages = soup.find('ul', id='pldb-table-subs')
    links = subpages.find_all('a')

    if '?all_attempts=1' not in url:
        url = url + '?all_attempts=1'

    div_name = re.sub('\(.*?\)', '', subpages.find('li').text).strip()

    subpage_html = gethtml(url)

    subpage_soup = BeautifulSoup(subpage_html, 'html.parser')
    csv = getpagedata(subpage_soup)

    # For some reason BirthYear sometimes vanishes when we show all attempts,
    # so grab it with the Russian names
    url = url.replace('?all_attempts=1', '')

    [russian_names, birthyears] = get_cyrillic_names_and_birthyears(url)
    csv.append_column('CyrillicName')
    cyrnameidx = csv.index('CyrillicName')
    nameidx = csv.index('Name')

    if 'BirthYear' not in csv.fieldnames:
        csv.append_column('BirthYear')
    byidx = csv.index('BirthYear')

    for ii in range(len(csv.rows)):
        if csv.rows[ii][nameidx] != russian_names[ii]:
            csv.rows[ii][cyrnameidx] = russian_names[ii]

        csv.rows[ii][byidx] = birthyears[ii]

    csv.append_column('SheetName')
    for row in csv.rows:
        row[-1] = div_name

    if len(links) > 1:
        for link in links:
            div_name = re.sub('\(.*?\)', '', link.text).strip()
            subpage_ext = link['href'].split('/')[:-1][-1]

            subpage_url = url + subpage_ext + '/?all_attempts=1'
            subpage_html = gethtml(subpage_url)

            subpage_soup = BeautifulSoup(subpage_html, 'html.parser')
            subpage_csv = getpagedata(subpage_soup)

            [russian_names, birthyears] = get_cyrillic_names_and_birthyears(
                subpage_url.replace('?all_attempts=1', ''))
            subpage_csv.append_column('CyrillicName')

            cyrnameidx = subpage_csv.index('CyrillicName')
            nameidx = subpage_csv.index('Name')

            if 'BirthYear' not in subpage_csv.fieldnames:
                subpage_csv.append_column('BirthYear')
            byidx = subpage_csv.index('BirthYear')

            for ii in range(len(subpage_csv.rows)):
                if subpage_csv.rows[ii][nameidx] != russian_names[ii]:
                    subpage_csv.rows[ii][cyrnameidx] = russian_names[ii]
  
                subpage_csv.rows[ii][byidx] = birthyears[ii]

            subpage_csv.append_column('SheetName')
            for row in subpage_csv.rows:
                row[-1] = div_name

            csv.cat(subpage_csv)

    return csv


# Returns just the Russian names and BirthYears for lifters
def get_cyrillic_names_and_birthyears(url):
    # Get the html for the Russian site
    html = gethtml(url.replace('http://en.', 'http://'))

    soup = BeautifulSoup(html, 'html.parser')

    names = []
    birthyears = []


# Get the results table.
    table = soup.find('table', {'class': 'pldb-table2'})
    if table == []:
        error("Couldn't find the results table.")

    trs = table.find_all('tr')

    # Get column information.
    headers = [x.text for x in trs[0].find_all('th')]
    nameidx = None
    byidx = None
    for ii in range(len(headers)):
        if headers[ii] == 'Спортсмен':
            nameidx = ii
        elif headers[ii] == 'ГР':
            byidx = ii

        if nameidx and byidx:
            break

    assert nameidx is not None

    for tr in trs[1:]:
        row = [x.text for x in tr.find_all('td')]

        if len(row) > 1:
            tds = tr.find_all('td')
            name = tds[nameidx].text

            names.append(name)

            if byidx:
                by = tds[byidx].text
            else:
                by = ''

            birthyears.append(by)

    return [names, birthyears]


def getpagedata(soup):
    csv = Csv()

    # Get the results table.
    table = soup.find('table', {'class': 'pldb-table2'})
    if table == []:
        error("Couldn't find the results table.")

    trs = table.find_all('tr')

    # Get column information.
    headers = [x.text for x in trs[0].find_all('th')]
    csv.fieldnames = []
    for h in headers:
        if h == ' ':
            csv.fieldnames += ['IGNORE']
        elif h == 'Pl':
            csv.fieldnames += ['Place']
        elif h == 'Lifter':
            csv.fieldnames += ['Name']
        elif h == 'Age cls':
            csv.fieldnames += ['Division']
        elif h == 'BY':
            csv.fieldnames += ['BirthYear']
        elif h == 'From':
            csv.fieldnames += ['IGNORE']
        elif h == 'Body wt':
            csv.fieldnames += ['BodyweightKg']
        elif h == 'SQ1':
            csv.fieldnames += ['Squat1Kg']
        elif h == 'SQ2':
            csv.fieldnames += ['Squat2Kg']
        elif h == 'SQ3':
            csv.fieldnames += ['Squat3Kg']
        elif h == 'SQ':
            csv.fieldnames += ['Best3SquatKg']
        elif h == 'BP1':
            csv.fieldnames += ['Bench1Kg']
        elif h == 'BP2':
            csv.fieldnames += ['Bench2Kg']
        elif h == 'BP3':
            csv.fieldnames += ['Bench3Kg']
        elif h == 'BP':
            csv.fieldnames += ['Best3BenchKg']
        elif h == 'DL1':
            csv.fieldnames += ['Deadlift1Kg']
        elif h == 'DL2':
            csv.fieldnames += ['Deadlift2Kg']
        elif h == 'DL3':
            csv.fieldnames += ['Deadlift3Kg']
        elif h == 'DL':
            csv.fieldnames += ['Best3DeadliftKg']
        elif h == 'Total':
            csv.fieldnames += ['TotalKg']
        elif h == 'Nat':
            csv.fieldnames += ['Country']
        elif h == 'G':
            csv.fieldnames += ['IGNORE']
        elif h in ['R','W']: # Resh, Wilks
            csv.fieldnames += ['IGNORE']
        else:
            error("Unknown column name: \"%s\"" % h)

    # These columns are added from the category rows.
    csv.fieldnames += ['WeightClassKg', 'Sex']

    wcstate = None
    sexstate = None

    for tr in trs[1:]:
        row = [x.text for x in tr.find_all('td')]

        # Rows of length >1 are actual results, of length 1 are categories.
        if len(row) == 1:
            # Extract sex information.
            text = row[0].lower()
            if 'women' in text:
                sexstate = 'F'
            elif 'men' in text:
                sexstate = 'M'
            elif 'category' in text:
                wcstate = text.replace('category ', '').replace(' kg', '')

        else:
            assert wcstate
            assert sexstate

            # Accumulate the row, but we need to look at the class of each td
            # to figure out whether lifts were good or bad.
            row = []
            for td in tr.find_all('td'):
                text = td.find(text=True)

                # Get the country data
                if text == None and td.has_attr('id') and 'LocationID' in td['id']:
                    text = td.a['title']

                if text == None:
                    text = ''

                spans = td.find_all('span')

                if len(spans) == 2 and spans[1]['class'] == ['result-value-fail']:
                    text = '-' + text

                row.append(text.strip().replace(
                    '  ', ' ').replace(',', ' ').replace('—', ''))

            row = row + [wcstate, sexstate]
            csv.rows += [row]

    return csv


def expand_fourths(csv, bestlift, lift4):
    bestidx = csv.index(bestlift)

    for row in csv.rows:
        if '(' in row[bestidx]:
            if lift4 not in csv.fieldnames:
                csv.insert_column(bestidx+1, lift4)
            lift4idx = csv.index(lift4)

            [row[bestidx], row[lift4idx]] = row[bestidx].split('(')
            row[bestidx] = row[bestidx].strip()
            row[lift4idx] = row[lift4idx].replace(')', '').strip()


def fixplace(csv):
    placeidx = csv.index('Place')
    totalidx = csv.index('TotalKg')
    for row in csv.rows:
        row[placeidx] = row[placeidx].replace('.', '')
        if row[placeidx] == '':
            row[placeidx] = 'DQ'
            row[totalidx] = ''  # Instead of a zero.
        elif row[placeidx] == 'DQ':  # Allpowerlifting marks doping disquals as DQ
            row[placeidx] = 'DD'
            row[totalidx] = ''  # Instead of a zero.


def fixtotals(csv):
    if 'TotalKg' not in csv.fieldnames:
        csv.insert_column(len(csv.fieldnames), 'TotalKg')

    totalidx = csv.index('TotalKg')
    eventidx = csv.index('Event')

    for row in csv.rows:
        total = 0
        if ('S' in row[eventidx] and 'Best3SquatKg' in csv.fieldnames and
                row[csv.index('Best3SquatKg')] != ''):
            total += float(row[csv.index('Best3SquatKg')])
        if ('B' in row[eventidx] and 'Best3BenchKg' in csv.fieldnames and
                row[csv.index('Best3BenchKg')] != ''):
            total += float(row[csv.index('Best3BenchKg')])
        if ('D' in row[eventidx] and 'Best3DeadliftKg' in csv.fieldnames and
                row[csv.index('Best3DeadliftKg')] != ''):
            total += float(row[csv.index('Best3DeadliftKg')])

        row[totalidx] = str(total)


def unreverse_names(csv):
    nameidx = csv.index('Name')
    cyrillicnameidx = csv.index('CyrillicName')

    for row in csv.rows:
        # Maiden names are given in brackets, remove these
        row[nameidx] = re.sub('\(.*?\)', '', row[nameidx]).strip()
        row[cyrillicnameidx] = re.sub(
            '\(.*?\)', '', row[cyrillicnameidx]).strip()

        parts = row[nameidx].split()
        if len(parts) > 1:
            fixed = [parts[-1]] + parts[:-1]
            row[nameidx] = ' '.join(fixed)

        parts = row[cyrillicnameidx].split()
        if len(parts) > 1:
            fixed = [parts[-1]] + parts[:-1]
            row[cyrillicnameidx] = ' '.join(fixed)


def markevent(csv):
    assert 'Event' not in csv.fieldnames
    csv.append_column('Event')

    evtidx = csv.index('Event')

    def getevtindices(csv, fieldl):
        indexlist = []
        for f in fieldl:
            try:
                indexlist.append(csv.index(f))
            except ValueError:
                pass
        return indexlist

    squatidxl = getevtindices(
        csv, ['Squat1Kg', 'Squat2Kg', 'Squat3Kg', 'Best3SquatKg'])
    benchidxl = getevtindices(
        csv, ['Bench1Kg', 'Bench2Kg', 'Bench3Kg', 'Best3BenchKg'])
    deadliftidxl = getevtindices(
        csv, ['Deadlift1Kg', 'Deadlift2Kg', 'Deadlift3Kg', 'Best3DeadliftKg'])

    for row in csv.rows:
        evt = ''
        for i in squatidxl:
            if row[i] != '':
                evt = evt + 'S'
                break
        for i in benchidxl:
            if row[i] != '':
                evt = evt + 'B'
                break
        for i in deadliftidxl:
            if row[i] != '':
                evt = evt + 'D'
                break
        row[evtidx] = evt

# Replace random english characters that AllPowerlifting uses with unicode alternatives


def fix_cyrillic_names(csv):
    if 'CyrillicName' in csv.fieldnames:
        cyridx = csv.index('CyrillicName')
        for row in csv.rows:
            row[cyridx] = row[cyridx].replace('e', 'е').replace('x', 'х')


def remove_empty_cols_ignore_fieldname(csv):
    def iscolempty(csv, i):
        for row in csv.rows:
            if row[i]:
                return False
        return True

    def getemptyidx(csv):
        for i, col in enumerate(csv.fieldnames):
            if iscolempty(csv, i):
                return i
        return -1

    while True:
        idx = getemptyidx(csv)
        if idx == -1:
            return
        csv.remove_column_by_index(idx)


def main(url):
    html = gethtml(url)

    soup = BeautifulSoup(html, 'html.parser')

    meetcsv = getmeetinfo(soup)
    dirname = getdirname(url)
    entriescsv = getresults(soup, url)
    if len(entriescsv.rows) == 0:
        error("No rows found!")

    if 'Best3SquatKg' in entriescsv.fieldnames:
        expand_fourths(entriescsv, 'Best3SquatKg', 'Squat4Kg')
    if 'Best3BenchKg' in entriescsv.fieldnames:
        expand_fourths(entriescsv, 'Best3BenchKg', 'Bench4Kg')
    if 'Best3DeadliftKg' in entriescsv.fieldnames:
        expand_fourths(entriescsv, 'Best3DeadliftKg', 'Deadlift4Kg')

    unreverse_names(entriescsv)

    fix_cyrillic_names(entriescsv)

    remove_empty_cols_ignore_fieldname(entriescsv)

    # Wilks will be automatically calculated later.
    # Feds get it wrong all the time.
    if 'Wilks' in entriescsv.fieldnames:
        entriescsv.remove_column_by_name('Wilks')

    # Figure out event information.
    markevent(entriescsv)

    fixtotals(entriescsv)
    fixplace(entriescsv)

    # Remove all the columns named 'IGNORE'
    while 'IGNORE' in entriescsv.fieldnames:
        entriescsv.remove_column_by_name('IGNORE')

    try:
        os.makedirs(dirname)
    except OSError as exception:
        if exception.errno != errno.EEXIST:
            raise
        else:
            error("Directory '%s' already exists." % dirname)

    with open(dirname + os.sep + 'entries.csv', 'w') as fd:
        entriescsv.write(fd)
    with open(dirname + os.sep + 'meet.csv', 'w') as fd:
        meetcsv.write(fd)
    with open(dirname + os.sep + 'URL', 'w') as fd:
        fd.write(url + "\n")

    print("Imported into %s." % dirname)


if __name__ == '__main__':
    if len(sys.argv) != 2:
        print("Usage: %s url" % sys.argv[0])
    main(sys.argv[1])
